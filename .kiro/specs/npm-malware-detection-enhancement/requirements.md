# Requirements Document

## Introduction

This specification defines the enhancement of the existing NodeWatch MVP into a comprehensive npm package malware detection system. The current system provides basic static analysis and mock LLM analysis for individual packages. This enhancement will transform it into a production-ready system capable of analyzing the top 1,000 npm packages with intelligent caching, real LLM integration, dynamic analysis capabilities, and a scalable architecture for future expansion to all 1.3M+ npm packages.

## Requirements

### Requirement 1: Enhanced Analysis Pipeline

**User Story:** As a security researcher, I want a multi-stage analysis pipeline that efficiently processes packages through static analysis, dynamic sandboxing, and targeted LLM analysis, so that I can get comprehensive security assessments while managing costs and processing time.

#### Acceptance Criteria

1. WHEN a package is submitted for analysis THEN the system SHALL process it through static analysis first
2. WHEN static analysis detects suspicious patterns THEN the system SHALL escalate to dynamic analysis
3. WHEN dynamic analysis produces ambiguous results THEN the system SHALL perform targeted LLM analysis on evidence bundles only
4. WHEN any analysis stage fails THEN the system SHALL log the error and continue with available results
5. IF a package has been analyzed before with the same content hash THEN the system SHALL return cached results
6. WHEN analysis is complete THEN the system SHALL calculate a weighted risk score from all stages

### Requirement 2: Real LLM Integration

**User Story:** As a security analyst, I want the system to use real AI models (OpenAI GPT-4, Claude, or local models) for code analysis, so that I can get intelligent assessments of suspicious code patterns that static analysis cannot definitively classify.

#### Acceptance Criteria

1. WHEN suspicious code is detected THEN the system SHALL create evidence bundles with context
2. WHEN sending to LLM THEN the system SHALL include suspicious code snippets, deobfuscated strings, and behavioral evidence
3. WHEN LLM analysis is requested THEN the system SHALL support multiple providers (OpenAI, Anthropic, local models)
4. IF LLM API fails THEN the system SHALL fall back to pattern-based scoring
5. WHEN LLM responds THEN the system SHALL parse structured JSON responses with confidence scores
6. WHEN processing costs exceed threshold THEN the system SHALL use local models or skip LLM analysis

### Requirement 3: Dynamic Analysis Sandbox

**User Story:** As a malware researcher, I want packages to be executed in isolated sandbox environments during analysis, so that I can observe actual runtime behavior including network calls, file operations, and process spawning without security risks.

#### Acceptance Criteria

1. WHEN dynamic analysis is triggered THEN the system SHALL create an isolated container environment
2. WHEN executing package install scripts THEN the system SHALL monitor file system operations
3. WHEN package code runs THEN the system SHALL capture network connection attempts
4. WHEN processes are spawned THEN the system SHALL log command execution and arguments
5. IF sandbox execution exceeds time limit THEN the system SHALL terminate and flag as suspicious
6. WHEN sandbox completes THEN the system SHALL analyze captured behaviors for malicious patterns

### Requirement 4: Content-Based Deduplication

**User Story:** As a system administrator, I want the system to avoid re-analyzing identical content across different packages and versions, so that processing is efficient and costs are minimized while maintaining comprehensive coverage.

#### Acceptance Criteria

1. WHEN extracting package files THEN the system SHALL calculate SHA-256 hashes for each file
2. WHEN a file hash exists in cache THEN the system SHALL reuse previous analysis results
3. WHEN storing analysis results THEN the system SHALL index by content hash not package version
4. WHEN dependencies are shared across packages THEN the system SHALL leverage cached dependency analysis
5. IF cache hit rate falls below 80% THEN the system SHALL optimize caching strategy
6. WHEN cache grows large THEN the system SHALL implement LRU eviction policy

### Requirement 5: Batch Processing System

**User Story:** As a security team lead, I want to process the top 1,000 npm packages in batches with queue management, so that I can systematically analyze popular packages while managing system resources and API rate limits.

#### Acceptance Criteria

1. WHEN batch processing starts THEN the system SHALL fetch the top 1K package list from the specified GitHub gist
2. WHEN packages are queued THEN the system SHALL prioritize by download count and dependency usage
3. WHEN processing packages THEN the system SHALL respect npm registry rate limits
4. WHEN API limits are reached THEN the system SHALL implement exponential backoff
5. IF a package analysis fails THEN the system SHALL retry up to 3 times with increasing delays
6. WHEN batch completes THEN the system SHALL generate a summary report with statistics

### Requirement 6: Enhanced Database Schema

**User Story:** As a data analyst, I want comprehensive storage of package metadata, analysis results, and dependency relationships, so that I can query historical data, track trends, and generate insights about the npm ecosystem's security posture.

#### Acceptance Criteria

1. WHEN storing packages THEN the system SHALL record complete npm registry metadata
2. WHEN saving analysis results THEN the system SHALL maintain version history and timestamps
3. WHEN processing dependencies THEN the system SHALL build and store dependency graphs
4. WHEN calculating scores THEN the system SHALL store individual signal weights and reasoning
5. IF database queries are slow THEN the system SHALL optimize with appropriate indexes
6. WHEN data grows large THEN the system SHALL implement partitioning strategies

### Requirement 7: Advanced Static Analysis

**User Story:** As a security engineer, I want enhanced static analysis that detects sophisticated malware patterns including obfuscation, typosquatting, and supply chain attack indicators, so that I can identify threats that simple pattern matching would miss.

#### Acceptance Criteria

1. WHEN analyzing code THEN the system SHALL detect multiple obfuscation techniques
2. WHEN checking package names THEN the system SHALL calculate typosquatting similarity scores
3. WHEN comparing tarballs to repositories THEN the system SHALL flag content mismatches
4. WHEN analyzing install scripts THEN the system SHALL detect suspicious lifecycle hooks
5. IF maintainer ownership changes THEN the system SHALL flag as potential supply chain risk
6. WHEN detecting encoded content THEN the system SHALL attempt deobfuscation before analysis

### Requirement 8: Comprehensive Risk Scoring

**User Story:** As a developer, I want transparent risk scores with detailed explanations, so that I can understand why a package received a particular security rating and make informed decisions about using it in my projects.

#### Acceptance Criteria

1. WHEN calculating risk scores THEN the system SHALL use weighted signals from all analysis stages
2. WHEN presenting scores THEN the system SHALL provide risk levels (Safe, Low, Medium, High, Critical)
3. WHEN explaining scores THEN the system SHALL list specific indicators and their confidence levels
4. WHEN scores change THEN the system SHALL maintain audit trail of score history
5. IF false positives are reported THEN the system SHALL allow score adjustments with justification
6. WHEN displaying results THEN the system SHALL show methodology transparency

### Requirement 9: API and Integration Layer

**User Story:** As a DevOps engineer, I want robust APIs for integrating package security checks into CI/CD pipelines, so that I can automatically scan dependencies during builds and block deployments of risky packages.

#### Acceptance Criteria

1. WHEN API requests are made THEN the system SHALL support REST endpoints with proper HTTP status codes
2. WHEN querying packages THEN the system SHALL provide pagination and filtering options
3. WHEN integrating with CI/CD THEN the system SHALL support webhook notifications
4. WHEN rate limiting is needed THEN the system SHALL implement per-user quotas
5. IF API usage is high THEN the system SHALL provide caching headers for client optimization
6. WHEN errors occur THEN the system SHALL return structured error responses with details

### Requirement 10: Monitoring and Observability

**User Story:** As a system operator, I want comprehensive monitoring of analysis performance, error rates, and system health, so that I can ensure reliable operation and optimize processing efficiency.

#### Acceptance Criteria

1. WHEN analysis runs THEN the system SHALL log processing times for each stage
2. WHEN errors occur THEN the system SHALL capture detailed error context and stack traces
3. WHEN system resources are used THEN the system SHALL monitor CPU, memory, and disk usage
4. WHEN API calls are made THEN the system SHALL track response times and error rates
5. IF performance degrades THEN the system SHALL alert operators with specific metrics
6. WHEN generating reports THEN the system SHALL provide analysis statistics and trends